"""
Test Centralized Conversion Utilities
====================================

Comprehensive tests for the single source of truth conversion system.
Tests both object and dict inputs to ensure proper typing.
"""

from datetime import datetime
from dataclasses import dataclass
from pydantic import BaseModel

from iointel.src.utilities.conversion_utils import (
    to_jsonable,
    tool_usage_results_to_llm,
    tool_usage_results_to_html,
    tool_catalog_to_llm_prompt,
    validation_errors_to_llm_prompt,
    execution_summary_to_llm_prompt,
    workflow_spec_to_llm_prompt
)
from iointel.src.agent_methods.data_models.workflow_spec import WorkflowSpec, NodeSpec, EdgeSpec, NodeData, EdgeData
from iointel.src.web.execution_feedback import WorkflowExecutionSummary, ExecutionStatus


class TestBasicConversions:
    """Test core conversion utilities."""
    
    def test_to_jsonable_with_objects(self):
        """Test conversion of various object types to JSON."""
        
        @dataclass
        class TestDataclass:
            name: str
            value: int
            timestamp: datetime
        
        class TestPydantic(BaseModel):
            title: str
            items: list
        
        # Test dataclass
        dc = TestDataclass("test", 42, datetime(2025, 1, 1, 12, 0, 0))
        result = to_jsonable(dc)
        assert result['name'] == "test"
        assert result['value'] == 42
        assert result['timestamp'] == "2025-01-01T12:00:00"
        
        # Test Pydantic model
        pm = TestPydantic(title="test", items=[1, 2, 3])
        result = to_jsonable(pm)
        assert result['title'] == "test"
        assert result['items'] == [1, 2, 3]
        
        # Test nested structures
        nested = {
            'data': dc,
            'model': pm,
            'list': [dc, pm],
            'plain': {'key': 'value'}
        }
        result = to_jsonable(nested)
        assert result['data']['name'] == "test"
        assert result['model']['title'] == "test"
        assert len(result['list']) == 2
        assert result['plain']['key'] == "value"


class TestToolUsageConversions:
    """Test tool usage result conversions with both objects and dicts."""
    
    def test_tool_results_objects_to_llm(self):
        """Test conversion of ToolUsageResult objects to LLM format."""
        
        @dataclass
        class MockToolResult:
            tool_name: str
            tool_args: dict
            result: str
        
        tool_results = [
            MockToolResult("searxng_search", {"query": "test"}, "Found 5 results"),
            MockToolResult("calculator_add", {"a": 10, "b": 5}, "15")
        ]
        
        llm_text = tool_usage_results_to_llm(tool_results)
        
        assert "1. **searxng_search**" in llm_text
        assert "2. **calculator_add**" in llm_text
        assert '"query": "test"' in llm_text
        assert "Found 5 results" in llm_text
        assert "15" in llm_text
    
    def test_tool_results_dicts_to_llm(self):
        """Test conversion of dict tool results to LLM format."""
        
        tool_results = [
            {
                "tool_name": "get_weather",
                "tool_args": {"city": "NYC"},
                "result": {"temp": 75, "condition": "sunny"}
            },
            {
                "tool_name": "send_email",
                "input": {"to": "test@example.com", "subject": "Test"},
                "tool_result": "Email sent successfully"
            }
        ]
        
        llm_text = tool_usage_results_to_llm(tool_results)
        
        assert "1. **get_weather**" in llm_text
        assert "2. **send_email**" in llm_text
        assert '"city": "NYC"' in llm_text
        assert '"temp": 75' in llm_text
        assert "Email sent successfully" in llm_text
    
    def test_tool_results_mixed_formats(self):
        """Test handling mixed object/dict formats gracefully."""
        
        @dataclass
        class MockResult:
            tool_name: str
            result: str
        
        mixed_results = [
            MockResult("tool1", "result1"),
            {"tool_name": "tool2", "result": "result2"},
            "invalid_format"  # Should handle gracefully
        ]
        
        llm_text = tool_usage_results_to_llm(mixed_results)
        
        assert "1. **tool1**" in llm_text
        assert "2. **tool2**" in llm_text
        assert "3. **invalid_format**" in llm_text
        assert "Unknown result format" in llm_text
    
    def test_tool_results_to_html(self):
        """Test HTML pill generation for tool results."""
        
        tool_results = [
            {
                "tool_name": "test_tool",
                "tool_args": {"param": "value"},
                "tool_result": "success"
            }
        ]
        
        html = tool_usage_results_to_html(tool_results)
        
        assert '<div class="tool-pill"' in html
        assert '🛠️ test_tool' in html
        assert '"param": "value"' in html
        assert 'success' in html
    
    def test_empty_tool_results(self):
        """Test handling of empty tool results."""
        
        assert tool_usage_results_to_llm([]) == "No tools used."
        assert tool_usage_results_to_html([]) == "<p>No tools used.</p>"


class TestToolCatalogConversions:
    """Test tool catalog conversions."""
    
    def test_tool_catalog_to_llm_prompt(self):
        """Test conversion of tool catalog to LLM prompt format."""
        
        catalog = {
            "searxng_search": {
                "description": "Search the web for information",
                "parameters": {
                    "query": {
                        "type": "string",
                        "required": True,
                        "description": "Search query string"
                    },
                    "limit": {
                        "type": "integer",
                        "required": False,
                        "description": "Maximum results"
                    }
                }
            },
            "calculator_add": {
                "description": "Add two numbers",
                "parameters": {
                    "a": {"type": "number", "required": True},
                    "b": {"type": "number", "required": True}
                }
            }
        }
        
        prompt = tool_catalog_to_llm_prompt(catalog)
        
        assert "# Available Tools:" in prompt
        assert "## searxng_search" in prompt
        assert "## calculator_add" in prompt
        assert "Search the web for information" in prompt
        assert "Add two numbers" in prompt
        assert "query: string (required)" in prompt
        assert "Search query string" in prompt
        assert "limit: integer" in prompt
    
    def test_empty_catalog(self):
        """Test handling of empty tool catalog."""
        
        assert tool_catalog_to_llm_prompt({}) == "No tools available."


class TestValidationErrorConversions:
    """Test validation error conversions."""
    
    def test_validation_errors_to_llm_prompt(self):
        """Test conversion of validation errors to LLM prompt."""
        
        errors = [
            ["Missing required field 'prompt' in user_input node"],
            ["Invalid edge: source node 'invalid_id' not found", "Edge target must exist in nodes"],
            ["SLA enforcement requires tools list"]
        ]
        
        prompt = validation_errors_to_llm_prompt(errors)
        
        assert "# Validation Errors Found:" in prompt
        assert "## Error Group 1:" in prompt
        assert "## Error Group 2:" in prompt
        assert "## Error Group 3:" in prompt
        assert "Missing required field 'prompt'" in prompt
        assert "Invalid edge: source node" in prompt
        assert "SLA enforcement requires" in prompt
        assert "## Required Fixes:" in prompt
    
    def test_empty_validation_errors(self):
        """Test handling of no validation errors."""
        
        assert validation_errors_to_llm_prompt([]) == "No validation errors."


class TestWorkflowConversions:
    """Test workflow specification conversions."""
    
    def test_workflow_spec_to_llm_prompt(self):
        """Test workflow spec conversion to LLM format."""
        
        # Create a simple workflow spec
        nodes = [
            NodeSpec(
                id="user_input",
                type="data_source",
                label="User Input",
                data=NodeData(source_name="user_input", config={"prompt": "Enter query"})
            ),
            NodeSpec(
                id="search_agent",
                type="agent", 
                label="Search Agent",
                data=NodeData(
                    agent_instructions="Search for information",
                    tools=["searxng_search"]
                )
            )
        ]
        
        edges = [
            EdgeSpec(
                id="e1",
                source="user_input",
                target="search_agent",
                data=EdgeData()
            )
        ]
        
        from uuid import uuid4
        
        spec = WorkflowSpec(
            id=uuid4(),
            rev=1,
            title="Test Search Workflow",
            description="A workflow that searches for information",
            nodes=nodes,
            edges=edges
        )
        
        prompt = workflow_spec_to_llm_prompt(spec)
        
        assert "# Workflow: Test Search Workflow" in prompt
        assert "Description: A workflow that searches for information" in prompt
        assert "## Nodes:" in prompt
        assert "- user_input (data_source): User Input" in prompt
        assert "- search_agent (agent): Search Agent" in prompt
        assert "Tools: searxng_search" in prompt
        assert "Instructions: Search for information" in prompt
        assert "## Edges:" in prompt
        assert "- user_input → search_agent" in prompt


class TestExecutionSummaryConversions:
    """Test execution summary conversions."""
    
    def test_execution_summary_to_llm_prompt(self):
        """Test execution summary conversion to LLM format."""
        
        # Create mock execution summary
        node_result = NodeExecutionResult(
            node_id="test_node",
            node_type="agent",
            node_label="Test Agent",
            status=ExecutionStatus.SUCCESS,
            started_at=datetime.now().isoformat(),
            finished_at=datetime.now().isoformat(),
            duration_seconds=1.5,
            result_preview="Task completed successfully",
            tool_usage=["searxng_search"]
        )
        
        summary = WorkflowExecutionSummary(
            execution_id="test-exec-123",
            workflow_id="workflow-123",
            workflow_title="Test Workflow",
            status=ExecutionStatus.SUCCESS,
            started_at=datetime.now().isoformat(),
            finished_at=datetime.now().isoformat(),
            total_duration_seconds=2.0,
            nodes_executed=[node_result],
            nodes_skipped=[],
            user_inputs={"query": "test search"},
            final_outputs={"result": "found information"}
        )
        
        prompt = execution_summary_to_llm_prompt(summary)
        
        assert "# Execution Report: Test Workflow" in prompt
        assert "Status: SUCCESS" in prompt
        assert "Duration: 2.00s" in prompt
        assert "Nodes Executed: 1" in prompt
        assert "Nodes Skipped: 0" in prompt
        assert "## Executed Nodes:" in prompt
        assert "✅ Test Agent (agent)" in prompt
        assert "Tools: searxng_search" in prompt
        assert "Result: Task completed successfully" in prompt


class TestLegacyCompatibility:
    """Test legacy compatibility aliases."""
    
    def test_legacy_format_result_for_html(self):
        """Test legacy format_result_for_html function."""
        
        from iointel.src.utilities.conversion_utils import format_result_for_html
        
        result_dict = {
            "result": "Agent completed the task",
            "tool_usage_results": [
                {"tool_name": "test_tool", "tool_args": {"param": "value"}, "tool_result": "success"}
            ]
        }
        
        html = format_result_for_html(result_dict)
        
        assert "<b>Agent:</b> Agent completed the task" in html
        assert "🛠️ test_tool" in html
        assert "success" in html


if __name__ == "__main__":
    # Run verbose smoke tests with detailed output
    print("🧪 Running conversion utilities smoke tests with verbose output...")
    print("=" * 80)
    
    # Test basic conversion
    print("\n1. Testing basic to_jsonable conversion:")
    test_data = {"name": "test", "value": 42, "nested": {"key": "value"}}
    json_result = to_jsonable(test_data)
    print(f"   Input:  {test_data}")
    print(f"   Output: {json_result}")
    assert json_result == test_data
    print("   ✅ Basic to_jsonable works correctly")
    
    # Test tool results conversion
    print("\n2. Testing tool usage results to LLM conversion:")
    tool_results = [
        {"tool_name": "searxng_search", "tool_args": {"query": "test"}, "result": "Found 5 results"},
        {"tool_name": "calculator", "input": {"a": 10, "b": 5}, "tool_result": "15"}
    ]
    llm_result = tool_usage_results_to_llm(tool_results)
    print(f"   Input: {tool_results}")
    print(f"   Output:\n{llm_result}")
    assert "1. **searxng_search**" in llm_result
    assert "2. **calculator**" in llm_result
    print("   ✅ Tool results to LLM conversion works")
    
    # Test HTML conversion
    print("\n3. Testing tool usage results to HTML conversion:")
    html_result = tool_usage_results_to_html(tool_results)
    print(f"   HTML length: {len(html_result)} chars")
    print(f"   Contains tool pills: {'tool-pill' in html_result}")
    print(f"   Contains tool names: {'searxng_search' in html_result and 'calculator' in html_result}")
    print("   ✅ Tool results to HTML conversion works")
    
    # Test catalog conversion
    print("\n4. Testing tool catalog to LLM prompt:")
    catalog = {
        "searxng_search": {
            "description": "Search the web for information", 
            "parameters": {
                "query": {"type": "string", "required": True, "description": "Search query"},
                "limit": {"type": "integer", "required": False}
            }
        },
        "calculator_add": {
            "description": "Add two numbers",
            "parameters": {
                "a": {"type": "number", "required": True},
                "b": {"type": "number", "required": True}
            }
        }
    }
    catalog_result = tool_catalog_to_llm_prompt(catalog)
    print(f"   Input catalog has {len(catalog)} tools")
    print(f"   Generated prompt:\n{catalog_result}")
    assert "## searxng_search" in catalog_result
    assert "## calculator_add" in catalog_result
    print("   ✅ Tool catalog conversion works")
    
    # Test validation errors
    print("\n5. Testing validation errors to LLM prompt:")
    errors = [
        ["Missing required field 'prompt' in user_input node"],
        ["Invalid edge: source node 'invalid_id' not found", "Edge target must exist in nodes"]
    ]
    validation_result = validation_errors_to_llm_prompt(errors)
    print(f"   Input errors: {errors}")
    print(f"   Generated prompt:\n{validation_result}")
    print("   ✅ Validation errors conversion works")
    
    # Test empty cases
    print("\n6. Testing empty/edge cases:")
    empty_tools = tool_usage_results_to_llm([])
    empty_catalog = tool_catalog_to_llm_prompt({})
    empty_errors = validation_errors_to_llm_prompt([])
    print(f"   Empty tools: '{empty_tools}'")
    print(f"   Empty catalog: '{empty_catalog}'") 
    print(f"   Empty errors: '{empty_errors}'")
    print("   ✅ Empty cases handled gracefully")
    
    print("\n" + "=" * 80)
    print("🎉 All verbose smoke tests passed! Centralized conversion utilities working correctly.")
    print("   - Proper object/dict typing ✅")
    print("   - Comprehensive test coverage ✅") 
    print("   - Legacy compatibility ✅")
    print("   - Single source of truth ✅")