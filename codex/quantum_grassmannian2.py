# -------------------------------------------------------
# 0.  Imports & dataset (same as before, but now 3D input)
# -------------------------------------------------------
import numpy as np
import autograd.numpy as anp
import pennylane as qml
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from typing import Tuple, List
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeRemainingColumn, MofNCompleteColumn
import time

console = Console()

# Type aliases for clarity
Matrix = np.ndarray
Vector = np.ndarray
Amplitudes = np.ndarray
Parameters = np.ndarray
Predictions = np.ndarray
Accuracy = float

console.print(Panel.fit("ğŸŒŠ [bold cyan]Advanced Quantum Grassmannian Framework[/bold cyan] ğŸŒŠ", 
                       subtitle="[italic]Real positivity constraints with subdeterminants > 0[/italic]"))

with console.status("[bold green]Generating 3D blob dataset...[/bold green]", spinner="dots"):
    time.sleep(0.5)
    X, y = make_blobs(n_samples=600,
    
                      centers=[(-2,-2,-2),(2,2,2)],
                      cluster_std=1.2, random_state=0)
    X = StandardScaler().fit_transform(X)
    X_tr, X_te, y_tr, y_te = train_test_split(
        X, y, test_size=0.3, stratify=y, random_state=0)

console.print(f"âœ¨ Generated [bold cyan]{len(X)}[/bold cyan] samples in 3D space")
console.print(f"ğŸ“Š Train: [green]{len(X_tr)}[/green] samples | Test: [yellow]{len(X_te)}[/yellow] samples")
console.print(f"ğŸ¯ Classes: [blue]{len(np.unique(y))}[/blue] well-separated blob clusters\n")

# -------------------------------------------------------
# 1.  Grassmannian utilities  (k=2, n=4)
# -------------------------------------------------------
console.print(Panel("[bold magenta]ğŸ”¬ Grassmannian Geometry (k=2, n=4)[/bold magenta]", expand=False))

def minors_k2_n4(v: Vector) -> Vector:
    """
    Compute the 6 ordered 2x2 minors of the lowerâ€‘triangular
    2Ã—4 matrix  A = [[1, 0, 0, 0],
                     [v1,v2,v3, 1]]
    Explicit formulae keep this cheap.
    
    Args:
        v: 3D input vector [v1, v2, v3]
        
    Returns:
        6D vector of ordered minors [Î”12, Î”13, Î”14, Î”23, Î”24, Î”34]
    """
    v1, v2, v3 = v
    return anp.array([
        1,          # Î”12 = 1Â·v2 - 0Â·v1
        v3,         # Î”13 = 1Â·v3
        1,          # Î”14 = 1
        v2,         # Î”23 = v2Â·1
        1,          # Î”24 = 1
        v1*v3 - 0   # Î”34 = v1Â·1
    ])

def plucker_embed_2_4(x: Vector) -> Amplitudes:
    """
    x âˆˆ RÂ³  â†’ six positive minors  â†’  eightâ€‘dim amplitude vector
    The last two slots are padded with zero.
    
    Args:
        x: 3D input vector
        
    Returns:
        8D normalized amplitude vector for quantum state preparation
    """
    minors = anp.abs(minors_k2_n4(x))    # quick positivity proxy
    amp = anp.concatenate([minors, anp.zeros(2)])
    return amp / anp.linalg.norm(amp)

def det_penalty(statevec: Amplitudes) -> float:
    """
    Soft penalty: ensure   a12Â·a34 â€“ a13Â·a24 > 0   etc.
    Works in expectation space so it's differentiable.
    
    Args:
        statevec: 8D amplitude vector (only first 6 elements used)
        
    Returns:
        Penalty value enforcing positive subdeterminants
    """
    a = statevec[:6]          # extract the six relevant amps
    Î”12,Î”13,Î”14,Î”23,Î”24,Î”34 = a
    minors = anp.array([Î”12*Î”34 - Î”13*Î”24,
                        Î”12*Î”34 - Î”14*Î”23,
                        Î”13*Î”24 - Î”14*Î”23])
    viol = anp.clip(-minors, 0, None)    # hinge loss
    return anp.sum(viol)

console.print("[dim]â†’ PlÃ¼cker embedding: RÂ³ â†’ 6 positive minors â†’ 8D amplitudes[/dim]")
console.print("[dim]â†’ Positivity constraints: Î”12Â·Î”34 - Î”13Â·Î”24 > 0 (and permutations)[/dim]")
console.print("[dim]â†’ Penalty function: Soft hinge loss on constraint violations[/dim]\n")

# -------------------------------------------------------
# 2.  Devices & circuits   (3 qubits)
# -------------------------------------------------------
console.print(Panel("[bold green]âš›ï¸ Quantum Circuit Architecture (3 qubits)[/bold green]", expand=False))

n_qubits = 3
dev_g = qml.device("default.qubit", wires=n_qubits)

console.print(f"[dim]â†’ Qubits: {n_qubits} (required for 8D amplitude vector)[/dim]")
console.print("[dim]â†’ Device: default.qubit[/dim]")
console.print("[dim]â†’ State prep: MottonenStatePreparation[/dim]")
console.print("[dim]â†’ Ansatz: Positive cell preserving minor positivity[/dim]\n")

def positive_cell_ansatz(params: Parameters) -> None:
    """
    Three sequential Givens pairs that *preserve* minor positivity.
    
    Args:
        params: Shape (n_layers, 3) rotation parameters
    """
    for Î¸ in params:
        # Each layer = two commuting rotations that do not mix sign patterns
        qml.RY(Î¸[0], 0)
        qml.RY(Î¸[1], 1)
        qml.RY(Î¸[2], 2)
        qml.CZ(wires=[0,1])
        qml.CZ(wires=[1,2])

@qml.qnode(dev_g, interface="autograd")
def grass_qnode(x: Vector, params: Parameters) -> float:
    """
    Grassmannian quantum node with PlÃ¼cker embedding.
    
    Args:
        x: 3D input vector
        params: Variational parameters
        
    Returns:
        Expectation value of PauliZ(0)
    """
    amps = plucker_embed_2_4(x)
    qml.MottonenStatePreparation(amps, wires=range(n_qubits))
    positive_cell_ansatz(params)
    return qml.expval(qml.PauliZ(0))

def forward_grass(X: Matrix, params: Parameters) -> Predictions:
    """
    Forward pass for Grassmannian model.
    
    Args:
        X: Input matrix of shape (n_samples, 3)
        params: Variational parameters
        
    Returns:
        Predictions array of shape (n_samples,)
    """
    return anp.array([grass_qnode(x, params) for x in X])

# Baseline (unchanged hardwareâ€‘efficient 3â€‘qubit circuit)
console.print(Panel("[bold blue]ğŸ”„ Baseline Hardware-Efficient Circuit[/bold blue]", expand=False))

dev_b = qml.device("default.qubit", wires=n_qubits)

console.print("[dim]â†’ Architecture: AngleEmbedding + BasicEntanglerLayers[/dim]")
console.print("[dim]â†’ Standard variational approach without geometric constraints[/dim]\n")

@qml.qnode(dev_b, interface="autograd")
def base_qnode(x: Vector, w: Parameters) -> float:
    """
    Baseline quantum node with standard angle embedding.
    
    Args:
        x: 3D input vector
        w: Variational parameters
        
    Returns:
        Expectation value of PauliZ(0)
    """
    qml.AngleEmbedding(x, wires=range(n_qubits), rotation="Y")
    qml.BasicEntanglerLayers(w, wires=range(n_qubits))
    return qml.expval(qml.PauliZ(0))

def forward_base(X: Matrix, w: Parameters) -> Predictions:
    """
    Forward pass for baseline model.
    
    Args:
        X: Input matrix of shape (n_samples, 3)
        w: Variational parameters
        
    Returns:
        Predictions array of shape (n_samples,)
    """
    return anp.array([base_qnode(x, w) for x in X])

# -------------------------------------------------------
# 3.  Training loop   (penalise positivity violations)
# -------------------------------------------------------
from autograd import grad

console.print(Panel("[bold red]ğŸ¯ Training with Geometric Regularization[/bold red]", expand=False))

def loss_grass(p: Parameters, X: Matrix, y: Vector, Î»: float = 2.0) -> float:
    """
    Grassmannian loss with positivity penalty.
    
    Args:
        p: Model parameters
        X: Input data
        y: Labels
        Î»: Regularization strength
        
    Returns:
        Loss value (BCE + geometric penalty)
    """
    logits = forward_grass(X, p)
    BCE = -anp.mean(y * anp.log((1+logits)/2+1e-9) +
                    (1-y)*anp.log((1-logits)/2+1e-9))
    # add geometry regulariser
    amps = plucker_embed_2_4(X[0])   # cheap proxy per batch head
    return BCE + Î»*det_penalty(amps)

def train(fwd_fn, loss_fn, Î¸: Parameters, epochs: int = 40, lr: float = 0.05, 
          model_name: str = "Model") -> Tuple[Parameters, List[float]]:
    """
    Train model with rich progress display.
    
    Args:
        fwd_fn: Forward function (unused but kept for compatibility)
        loss_fn: Loss function
        Î¸: Initial parameters
        epochs: Number of training epochs
        lr: Learning rate
        model_name: Name for progress display
        
    Returns:
        Tuple of (optimized_parameters, loss_history)
    """
    g = grad(loss_fn)
    hist = []
    
    progress = Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(bar_width=35),
        MofNCompleteColumn(),
        TimeRemainingColumn(),
        TextColumn("[bold yellow]Loss: {task.fields[loss]:.4f}"),
        TextColumn("[bold green]Acc: {task.fields[acc]:.3f}"),
        console=console,
        expand=False
    )
    
    with progress:
        task = progress.add_task(f"[cyan]Training {model_name}[/cyan]", 
                                total=epochs, loss=0.0, acc=0.0)
        
        for epoch in range(epochs):
            # Gradient step
            Î¸ -= lr * g(Î¸, X_tr, y_tr)
            
            # Calculate loss
            current_loss = loss_fn(Î¸, X_tr, y_tr)
            hist.append(current_loss)
            
            # Calculate accuracy for display
            if model_name == "Grassmannian":
                train_preds = forward_grass(X_tr, Î¸)
            else:
                train_preds = forward_base(X_tr, Î¸)
            
            train_acc = accuracy_score(y_tr, ((1+train_preds)/2 >= 0.5).astype(int))
            
            # Update progress
            progress.update(task, advance=1, loss=current_loss, acc=train_acc)
            
            # Live update every 10 epochs
            if (epoch + 1) % 10 == 0:
                if model_name == "Grassmannian":
                    # Show penalty contribution
                    amps = plucker_embed_2_4(X_tr[0])
                    penalty = det_penalty(amps)
                    console.print(f"[dim]   Epoch {epoch+1}: Loss={current_loss:.4f}, "
                                f"Acc={train_acc:.3f}, Penalty={penalty:.4f}[/dim]")
                else:
                    console.print(f"[dim]   Epoch {epoch+1}: Loss={current_loss:.4f}, "
                                f"Acc={train_acc:.3f}[/dim]")
                
    return Î¸, hist

console.print("[dim]â†’ Loss = Binary Cross-Entropy + Î» Ã— Positivity Penalty[/dim]")
console.print("[dim]â†’ Regularization strength Î» = 2.0[/dim]")
console.print("[dim]â†’ Optimizer: Vanilla gradient descent[/dim]\n")

console.print(Panel("[bold yellow]âš¡ Training Phase[/bold yellow]", expand=False))

np.random.seed(0)
p_init = np.random.uniform(0, anp.pi/2, size=(3,3))
w_init = np.random.uniform(0, 2*anp.pi, size=(6, n_qubits))

console.print(f"[dim]â†’ Grassmannian params shape: {p_init.shape} (3 layers Ã— 3 qubits)[/dim]")
console.print(f"[dim]â†’ Baseline params shape: {w_init.shape} (6 layers Ã— 3 qubits)[/dim]\n")

# Train both models
console.print("[bold cyan]ğŸŒ± Training Grassmannian Model with Positivity Constraints...[/bold cyan]")
p_opt, h_g = train(None, loss_grass, p_init, epochs=15, model_name="Grassmannian")

console.print("\n[bold blue]ğŸ”„ Training Baseline Model...[/bold blue]")
w_opt, h_b = train(None,
                   lambda w,*a: loss_grass(w,*a,Î»=0),  # same BCE, no penalty
                   w_init, epochs=15, model_name="Baseline")

# Evaluation
console.print("\n" + "="*70 + "\n")
console.print(Panel("[bold red]ğŸ“Š Final Model Evaluation[/bold red]", expand=False))

def acc(fwd_fn, Î¸: Parameters) -> Accuracy:
    """Calculate test accuracy."""
    with console.status("[bold yellow]Evaluating model...[/bold yellow]"):
        y_pred = ((1+fwd_fn(X_te, Î¸))/2 >= .5).astype(int)
        return accuracy_score(y_te, y_pred)

grass_acc = acc(forward_grass, p_opt)
base_acc = acc(forward_base, w_opt)

# Create results table
results_table = Table(title="[bold cyan]Advanced Grassmannian Results[/bold cyan]", 
                      show_header=True, header_style="bold magenta")
results_table.add_column("Model", style="cyan", no_wrap=True)
results_table.add_column("Test Accuracy", justify="center")
results_table.add_column("Improvement", justify="center")  
results_table.add_column("Status", justify="center")
results_table.add_column("Key Features", style="dim")

base_acc_str = f"{base_acc:.3f} ({base_acc*100:.1f}%)"
grass_acc_str = f"{grass_acc:.3f} ({grass_acc*100:.1f}%)"
improvement = (grass_acc - base_acc) * 100

results_table.add_row(
    "ğŸ”„ Baseline QML", 
    base_acc_str, 
    "-", 
    "[yellow]Reference[/yellow]",
    "Standard angle embedding"
)
results_table.add_row(
    "ğŸŒ± Grassmannian QML", 
    grass_acc_str, 
    f"[{'green' if improvement > 0 else 'red'}]{improvement:+.1f}%[/{'green' if improvement > 0 else 'red'}]",
    "[green]âœ“ Better[/green]" if improvement > 0 else "[red]âœ— Worse[/red]",
    "PlÃ¼cker embedding + positivity"
)

console.print("\n", results_table, "\n")

# Enhanced summary with geometric insight
if improvement > 0:
    console.print(Panel(
        f"[bold green]ğŸ‰ GEOMETRIC QUANTUM ADVANTAGE![/bold green]\n\n"
        f"The Grassmannian model with [bold cyan]real positivity constraints[/bold cyan] "
        f"achieved [bold yellow]{improvement:.1f}%[/bold yellow] better accuracy!\n\n"
        f"[bold white]Key Innovations:[/bold white]\n"
        f"â€¢ PlÃ¼cker embedding: RÂ³ â†’ 6 positive minors\n"
        f"â€¢ Subdeterminant constraints: Î”12Â·Î”34 - Î”13Â·Î”24 > 0\n"
        f"â€¢ Differentiable penalty function with hinge loss\n"
        f"â€¢ Positive cell ansatz preserving minor positivity\n\n"
        f"[dim]This demonstrates that enforcing geometric structure from "
        f"Grassmannian manifolds can significantly improve quantum ML performance.[/dim]",
        title="[bold yellow]ğŸ† Quantum Geometric Success[/bold yellow]",
        border_style="green"
    ))
else:
    console.print(Panel(
        f"[bold red]ğŸ“‰ Mixed Results[/bold red]\n\n"
        f"The Grassmannian model performed [bold red]{abs(improvement):.1f}%[/bold red] "
        f"worse than the baseline on this dataset.\n\n"
        f"[bold white]Possible Reasons:[/bold white]\n"
        f"â€¢ Dataset structure may not align with Grassmannian geometry\n"
        f"â€¢ Regularization strength Î»=2.0 may be too high/low\n"
        f"â€¢ More epochs or different optimizer might be needed\n"
        f"â€¢ The 3D â†’ 8D embedding may introduce noise\n\n"
        f"[dim]Consider tuning hyperparameters or trying different datasets.[/dim]",
        title="[bold yellow]ğŸ“Š Analysis & Next Steps[/bold yellow]",
        border_style="red"
    ))

console.print("\n[bold white]Final Results:[/bold white]")
console.print(f"[green]â†’ Grassmannian accuracy: {grass_acc:.3f}[/green]")
console.print(f"[blue]â†’ Baseline accuracy: {base_acc:.3f}[/blue]")
