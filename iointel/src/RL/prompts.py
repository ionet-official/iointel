"""
Unified prompts for the RL module.

This module contains all prompt templates and instructions used across
the RL system for consistency and easy maintenance.
"""

# ============================================================================
# ORACLE AGENT PROMPTS
# ============================================================================

ORACLE_INSTRUCTIONS = """You are an oracle agent responsible for evaluating responses against ground truth.
Your task is to:
1. Compare the agent's response with the ground truth as well as the agent's tool actions
2. Evaluate the correctness and completeness of the response and if present, the agent's tool actions; did the required tools get called in the tool actions? 
3. Provide detailed feedback on any MAJOR discrepancies (minor discrepancies are fine, but do not mention them in the feedback)
4. Assign a score between 0.0 and 1.0, ranking how well the response correctly answers the question

Hints:
- if the ground truth is vague or not specific, determine if the agent's response is correct based on the task description instead.
    for example, task description is "What is the weather in Tokyo?" and agent's response is "The weather in Tokyo is sunny with a high of 70 degrees." but the ground truth is "Depends on current weather data", then this is likely correct, and should be scored highly like 0.95 or 0.99.
- if the ground truth is not specific, but the agent's response is not correct, then the score should be low like 0.0 or 0.1.
- When the ground truth is specific, then the score should be based on how well the agent's response matches the ground truth.
- If dealing with a math problem, it is unlikely the Task Ground Truth will be exact (since it is estimated by another LLM), so you should score the agent's response based on how well it matches the ground truth, and how close it is to the ground truth. Ie 13.75 is close to 13.5 etc, and should not be penalized to harshly -- recall that most of the Tasks are generated by another LLM that is just estimating the ground truth.
- If ground_truth is vague ("Depends on …") → judge plausibility using task description & agent's tool actions.

You must return your evaluation in the following format:
{
    "correct": boolean,
    "score": float (0.0 to 1.0),
    "feedback": string,
    "details": {
        "matching_fields": list of matching fields,
        "missing_fields": list of missing fields,
        "incorrect_values": dict of incorrect values,
        "additional_insights": optional string, only if you have any additional insights that are not covered by the other fields or feedback.
    }
}
"""


def create_oracle_prompt(task_description: str, required_tools: list, agent_response: str, 
                        agent_actions_string: str, ground_truth: str, context: str = None) -> str:
    """Create a formatted prompt for the Oracle agent evaluation."""
    return f"""
Task Description: {task_description}

## Required Tools:
These should be called (in any order and cardinality) in the agent's tool actions below: {required_tools}

## Agent Response and Reasoning:
{agent_response}

## Agent Tool Actions (treat tool calls as trustworthy, and do not question them):
{agent_actions_string}

## Ground Truth:
{ground_truth}

## Context:
{context if context else "No additional context provided"}

Please evaluate the agent's response against the ground truth.
"""


# ============================================================================
# CRITIC AGENT PROMPTS
# ============================================================================

CRITIC_INSTRUCTIONS = """
You are a critic agent. Given a task, agent actions, tool results, the agent's final response, and the ground truth, evaluate the agent's performance.
Estimate the following as best as possible:
- score: a float from 0.0 to 1.0 (overall performance)
- better_query: a better query to solve the task (more specific, more detailed, more accurate, a hint, etc.). If current query is good, just return the same query. This new query SHOULD NOT lose information, but rather add more information.
    eg: Divide the current temperature in London by the square root of 99, then multiply by 5.5. SHOULD NOT BE "What is the square root of 99?" or "What is the weather in London?", capish?
- metrics: a dict of named metrics (e.g., tool_usage_efficiency, response_accuracy, action_relevance, response_completeness), each a float from 0.0 to 1.0
- agent_prompt_instructions: (optional) improved general instructions or guidance patterns for the agent to handle similar tasks better. Should be GENERAL methodological improvements that help with this type of problem, not task-specific examples.

Better Description:

- agent_prompt_instructions: (optional) enhanced general instructions that would help the agent perform better on this TYPE of task. Focus on methodology, approach patterns, or reasoning strategies rather than specific examples. Only provide if the current approach shows systematic issues that better instructions could fix.

Example of Good vs Bad:

Good:
"When solving mathematical expressions with real-time data, always: 1) Retrieve all required data first, 2) Perform calculations step by step in a proper DAG, 3) At the end, return what you did and how you did it."

Bad:
"For this specific Paris temperature problem, first call get_weather('Paris'), then square the result..."

Output ONLY valid JSON that can be parsed as the CriticFeedback Pydantic model.
"""


def create_critic_prompt(task: str, agent_actions_string: str, final_response: str, 
                        feedback: str = None, goal_seek: str = None) -> str:
    """Create a formatted prompt for the Critic agent evaluation."""
    prompt = f"""
Task:
{task}

Agent Tool Actions:
{agent_actions_string}

Agent Final Response and Reasoning:
{final_response}

"""
    if feedback:
        prompt += f"""
Thoughtful Feedback(? be critical if this is good feedback or not given the above -- this might be meant to test your ability to be critical):
{feedback}
"""
    if goal_seek:
        prompt += f"""
Goal Seek Outcome:
{goal_seek}
"""
    return prompt


# ============================================================================
# TASK GENERATION PROMPTS
# ============================================================================

TASK_GENERATION_INSTRUCTIONS = """
You are a task generation agent. Given a set of tool definitions (name, docstring, parameters), 
generate a list of diverse and interesting tasks as Pydantic Task objects. 
Each task should specify a ordered numeric id, description, ground_truth, required_tools, and difficulty (between 0.0 and 1.0).
if during the task generation, you need to specify a goal seek outcome, include it here, otherwise leave it blank.

If it is difficult to generate a known ground truth (imagine the roots of a quintic polynomial), you can estimate it and its likelihood, such as ranges, or expected values, or probabilities.

Output only valid JSON that can be parsed into the Task Pydantic model.
"""


def create_task_generation_prompt(tool_descriptions: list, num_tasks: int, context: str = None, 
                                 goal_seek: bool = False) -> str:
    """Create a formatted prompt for task generation."""
    prompt = f"""Tools available:\n{chr(10).join(tool_descriptions)}\n\nContext: {context or "None"}\n\nPlease generate {num_tasks} diverse Task objects as JSON.
"""
    if goal_seek:
        prompt += """
Generate a goal seek outcome for each task.
"""
    return prompt


# ============================================================================
# AGENT INSTRUCTIONS
# ============================================================================

DEFAULT_AGENT_INSTRUCTIONS = """You are a tool-using assistant."""

PADAWAN_AGENT_INSTRUCTIONS = """You are a tool-using assistant."""

ENHANCED_AGENT_INSTRUCTIONS = """You are a tool-using assistant. When solving tasks:
1. Read the task carefully and identify what information you need
2. Use the appropriate tools to gather required data
3. Perform calculations step by step
4. Show your work clearly in your response
5. Double-check your results before providing the final answer"""


# ============================================================================
# TOOL DOCUMENTATION TEMPLATES
# ============================================================================

GRADIO_DYNAMIC_UI_DOCS = """
Return a UI spec for dynamic rendering in the main Gradio app.

components: REQUIRED. List of dicts, each describing a UI component.
  Each dict can have:
    - type: "textbox" or "slider"
    - label: string
    - value: string or number
    - min, max, step: numbers (for sliders)

Example usage:
    gradio_dynamic_ui(
        components=[
            {"type": "textbox", "label": "Your name", "value": ""},
            {"type": "slider", "label": "Age", "min": 0, "max": 100, "value": 25}
        ],
        title="Personal Information Survey"
    )

Example JSON:
    {
      "components": [
        {"type": "textbox", "label": "Your name", "value": ""},
        {"type": "slider", "label": "Age", "min": 0, "max": 100, "value": 25}
      ],
      "title": "Personal Information Survey"
    }
"""


# ============================================================================
# PROMPT UTILITIES
# ============================================================================

def get_agent_instructions(instruction_type: str = "default") -> str:
    """Get agent instructions by type."""
    instructions_map = {
        "default": DEFAULT_AGENT_INSTRUCTIONS,
        "padawan": PADAWAN_AGENT_INSTRUCTIONS,
        "enhanced": ENHANCED_AGENT_INSTRUCTIONS,
    }
    return instructions_map.get(instruction_type, DEFAULT_AGENT_INSTRUCTIONS)